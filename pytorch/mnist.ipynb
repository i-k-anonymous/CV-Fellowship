{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with PyTorch\n",
    "In this notebook, we will implement a fully connected network that classifies\n",
    "handwritten digits.\n",
    "\n",
    "This time, we will use the torchvision mnist dataset. The underlying data is\n",
    "the same as in the Keras version, but torchvision is easier to interface with\n",
    "from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = torch.device(\"cpu\") # Put your device string here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's setup our dataset, as we did in the micrograd example. In addition,\n",
    "we will visualize a sample data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = MNIST(\"data\", download=True, train=True, transform=transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.1307,), (0.3081,))\n",
    "]))\n",
    "test_loader = MNIST(\"data\", download=True, train=False, transform=transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.1307,), (0.3081,))\n",
    "]))\n",
    "plt.imshow(train_loader[0][0][0], cmap=\"gray\")\n",
    "plt.title(f\"Ground truth={train_loader[0][1]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first implement our model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Note the use of nn.Sequential here for convenience.\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(784, 800),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(800, 10),\n",
    "      nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x) # Using nn.Sequential makes this easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will instantiate our model, loss function, and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, now that we have a much more powerful engine, we can actually train\n",
    "our model!\n",
    "\n",
    "Specifically note that instead of performing full-batch gradient descent, like\n",
    "we did in `iris.ipynb`, we are using \"mini-batch\" gradient descent, so that we\n",
    "aren't training on the entire dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "  for i, (x, y) in enumerate(train_loader):\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  print(f\"Epoch {epoch} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Accuracy:\", sum(torch.argmax(model(x.to(DEVICE)), dim=1) == y.to(DEVICE) for x, y in test_loader) / len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, try running it on some test examples, and see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_IDX = 0\n",
    "x, y = test_loader[SAMPLE_IDX]\n",
    "plt.imshow(x[0], cmap=\"gray\")\n",
    "plt.title(f\"Ground truth={y}\")\n",
    "pred = torch.argmax(model(x.to(DEVICE)))\n",
    "plt.xlabel(f\"Prediction={pred}\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-fellowship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
